{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/dbl/.pyenv/versions/3.6.9/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#不要語削除\n",
    "def chenge_text(text):\n",
    "    text = text.translate(str.maketrans( '', '',string.punctuation))\n",
    "    text = re.sub(r'[.]{2,}','.',text)\n",
    "    text = re.sub(r'[\t]',' ',text)\n",
    "    text = re.sub(r'[ ]{2,}',' ',text)\n",
    "    return text\n",
    "\n",
    "def docvec(row):\n",
    "    feature_vec = np.zeros((300,),dtype='float32') \n",
    "    words = row.split(\" \")\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        try: \n",
    "            vec = model.wv[word]\n",
    "            feature_vec += vec\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if count == 0:\n",
    "        return feature_vec\n",
    "    else:\n",
    "        return feature_vec/count\n",
    "\n",
    "with open('../data'+os.sep+'dbpedia'+os.sep+'dbpedia_csv'+os.sep+'classes.txt','r',encoding='utf-8') as f:\n",
    "    classes = []\n",
    "    reader = f.read().splitlines()\n",
    "    for c in reader:\n",
    "        classes.append(docvec(c))\n",
    "\n",
    "with open('../data'+os.sep+'dbpedia'+os.sep+'dbpedia_csv'+os.sep+'test.csv','r',encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    for row in reader:\n",
    "        text_stock = []\n",
    "        text = row[2][1:].replace('(',')').split(')')\n",
    "        for i,t in enumerate(text):\n",
    "            if i % 2 == 0:\n",
    "                text_stock.append(t)\n",
    "        text = ''.join(text_stock)\n",
    "        text = chenge_text(text)\n",
    "        text = ' '.join([x for x in text.split(' ') if x not in row[1].split(' ')])\n",
    "        text = text.replace('  ',' ')\n",
    "        x_test.append(docvec(chenge_text(text)))\n",
    "        y_test.append(int(row[0])-1)\n",
    "\n",
    "np.save('../dataset/test/x_test.npy',x_test)\n",
    "np.save('../dataset/test/y_test.npy',y_test)\n",
    "np.save('../dataset/test/classes.npy',classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0      0.854     0.416     0.560      5000\n           1      0.501     0.755     0.602      5000\n           2      0.861     0.331     0.479      5000\n           3      0.968     0.645     0.774      5000\n           4      0.432     0.349     0.386      5000\n           5      0.423     0.473     0.447      5000\n           6      0.611     0.474     0.534      5000\n           7      0.201     0.496     0.286      5000\n           8      0.643     0.984     0.778      5000\n           9      0.754     0.064     0.118      5000\n          10      0.615     0.485     0.542      5000\n          11      0.785     0.931     0.852      5000\n          12      0.767     0.739     0.753      5000\n          13      0.509     0.624     0.561      5000\n        1000      0.000     0.000     0.000         0\n\n    accuracy                          0.555     70000\n   macro avg      0.595     0.518     0.511     70000\nweighted avg      0.637     0.555     0.548     70000\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rank1(vec,class_list):\n",
    "    min=-1\n",
    "    pp=1000\n",
    "    for j in range(len(class_list)):\n",
    "        vec1 = vec\n",
    "        vec2 = class_list[j]\n",
    "        cos = np.sum((vec1*vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
    "        if cos>min:\n",
    "            min=cos\n",
    "            pp=j\n",
    "    return pp\n",
    "\n",
    "x_test = np.load('../dataset/test/x_test.npy')\n",
    "y_test = np.load('../dataset/test/y_test.npy')\n",
    "classes = np.load('../dataset/test/classes.npy')\n",
    "\n",
    "pred = []\n",
    "for i,vec in zip(y_test,x_test):\n",
    "    pp = rank1(vec,classes)\n",
    "    pred.append(pp)\n",
    "\n",
    "from sklearn import metrics\n",
    "rep = metrics.classification_report(y_test,pred,digits=3)\n",
    "print(rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('3.6.9')",
   "metadata": {
    "interpreter": {
     "hash": "dacf6aa3b6aaa6166c9aab738ca0f053c223d2fabaa97df58850caa710ae1ba3"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}